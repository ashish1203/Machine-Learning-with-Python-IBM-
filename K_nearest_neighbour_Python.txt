#KNN- K-Nearest Neighbour :It considers the 'K' Nearest Neighbors (points) 
#when it predicts the classification of the test point.


import numpy as np                                    # array operations
import matplotlib.pyplot as plt
import pandas as pd                                   # reading csv file
from sklearn import preprocessing                     # used for normalization
from sklearn.model_selection import train_test_split  # splitting the data
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics                           # measuring accuracy


df = pd.read_csv('teleCust1000t.csv')  # 1000 by 12 data
#df.head()
#print(np.shape(df))
#df['custcat']  # target variable
df['custcat'].value_counts() # it gives the count of numbers 
#df.hist(column='income', bins=50)

#feature set
df.columns

# convert pandas dataframe into numpy array
X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)

#X[0:5] # first 5 rows data in array format

y = df['custcat'].values
#y[0:5] # first 5 values 

# Normalization
#Why it is required? - The nearness of samples is typically based on Euclidean distance.
#Suppose you had a dataset (m "examples" by n "features") and all 
#but one feature dimension had values strictly between 0 and 1, while a single feature dimension 
#had values that range from -1000000 to 1000000. When taking the euclidean distance between pairs 
#of "examples", the values of the feature dimensions that range between 0 and 1 may become uninformative
#and the algorithm would essentially rely on the single dimension whose values are substantially larger.
# Normalization solves this problem!

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))

#X[0:5]


# data split

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=5)
print ('Total training data:', X.shape)
print ('Total test data:', y.shape)
print ('Train set X and y:', X_train.shape,  y_train.shape)
print ('Test set X and y:', X_test.shape,  y_test.shape)

X_train[0:2]

#create model
k = 4
#Train Model 
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train) # training data of features and test data
neigh

# Predict
y_pre = neigh.predict(X_test)
print(y_pre[0:2])
print(y_test[0:2])


#Accuracy

# test with training data itself
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))

# test with test data itself
print("Test set Accuracy: ", metrics.accuracy_score(y_test, y_pre))
