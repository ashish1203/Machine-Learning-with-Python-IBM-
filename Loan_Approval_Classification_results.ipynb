{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard_similarity_score of LR =  0.6145833333333334\n",
      "f1_score of LR =  0.4837365591397849\n",
      "log_loss of LR =  13.312111588768252\n",
      "jaccard_similarity_score of DC =  0.7291666666666666\n",
      "f1_score of DC =  0.699702380952381\n",
      "log_loss of DC =  9.354435181366064\n",
      "jaccard_similarity_score of SVM =  0.6458333333333334\n",
      "f1_score of SVM =  0.5171830484330484\n",
      "log_loss of SVM =  12.232766497287395\n",
      "jaccard_similarity_score of KNN =  0.6458333333333334\n",
      "f1_score of KNN =  0.5917245370370371\n",
      "log_loss of KNN =  12.232716522448007\n"
     ]
    }
   ],
   "source": [
    "#Load libraries\n",
    "import pandas as pd # for df operation\n",
    "import numpy as np  # for array\n",
    "\n",
    "from sklearn import model_selection # train-test split\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#Load dataset from the Github\n",
    "url = 'https://raw.githubusercontent.com/ashish1203/Machine-Learning-with-Python-IBM-/master/Loan-applicant-details_1.csv'\n",
    "names = ['Loan_ID','Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History','Property_Area','Loan_Status']\n",
    "df = pd.read_csv(url, names=names)\n",
    "\n",
    "# visualize the data\n",
    "#print(df.head(5)) # First 5 elements of the data\n",
    "#print(df.describe()) # this will give the statistic data of data\n",
    "#print(np.shape(df)) ## overall size data\n",
    "\n",
    "# Check the type of data- Is any categorical data is there or not \n",
    "#print(df.dtypes) # this will give idea abt the data types involved in the dataset - row-wise\n",
    "\n",
    "\n",
    "# Filter the data: changing the categorical data  to numerical value\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = ['Loan_ID', 'Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i]) # encoding starts from 0\n",
    "    \n",
    "#again check the data\n",
    "#print(df.dtypes)\n",
    "\n",
    "\n",
    "#Split the dataset into 80% training and 20% test data\n",
    "# for spliting the list - we first convert into array \n",
    "array = df.values\n",
    "X = array[:,6:11] # all rows but 6:11 columns for training\n",
    "X=X.astype('int')\n",
    "Y = array[:,12] # 12 column select for - prediction\n",
    "\n",
    "Y=Y.astype('int')\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2, random_state=4)\n",
    "\n",
    "#print(x_train.shape) # 384 by 5\n",
    "#print(x_test.shape) # 96 by 5\n",
    "#print(y_train.shape) # 384\n",
    "#print(y_test.shape) # 96\n",
    "\n",
    "#####################################################################################################################################################################\n",
    "# Training of the Model\n",
    "\n",
    "# (1) Logistic Regression\n",
    "\n",
    "# Model the logistic regression\n",
    "# C:C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization.\n",
    "# solver:numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.\n",
    "# Regularization is a technique used to solve the overfitting problem in machine learning models.\n",
    "\n",
    "model = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)\n",
    "model\n",
    "#model = LogisticRegression()\n",
    "#model.fit(x_train,y_train)\n",
    "# Prediction of LR\n",
    "predictions_LR = model.predict(x_test)\n",
    "\n",
    "# Accuracy of LR\n",
    "#(1) JACCARD index\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print('jaccard_similarity_score of LR = ', jaccard_similarity_score(y_test, predictions_LR))\n",
    "\n",
    "# (2) F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1_score of LR = ',f1_score(y_test, predictions_LR, average='weighted'))\n",
    "\n",
    "\n",
    "# (3) LogLoss\n",
    "# a higher log-loss value means better predictions. \n",
    "# Log loss( Logarithmic loss) measures the performance of a \n",
    "#classifier where the predicted output is a probability value between 0 and 1.\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('log_loss of LR = ',log_loss(y_test, predictions_LR, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
    "\n",
    "##############################################################################################################################################################################\n",
    "# (2) Decision Tree :We choose the Entropy as a model selection paramter.\n",
    "\n",
    "model_DC = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
    "# fit the training data\n",
    "model_DC.fit(x_train,y_train)\n",
    "\n",
    "#Prediction of DC\n",
    "predictions_DC = model_DC.predict(x_test)\n",
    "\n",
    "# Accuracy of DC\n",
    "#(1) JACCARD index\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print('jaccard_similarity_score of DC = ', jaccard_similarity_score(y_test, predictions_DC))\n",
    "\n",
    "# (2) F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1_score of DC = ',f1_score(y_test, predictions_DC, average='weighted'))\n",
    "\n",
    "\n",
    "# (3) LogLoss\n",
    "# a higher log-loss value means better predictions. \n",
    "# Log loss( Logarithmic loss) measures the performance of a \n",
    "#classifier where the predicted output is a probability value between 0 and 1.\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('log_loss of DC = ',log_loss(y_test, predictions_DC, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
    "\n",
    "############################################################################################################################################################################\n",
    "\n",
    "# (3) Support vector machine\n",
    "# Model the SVM\n",
    "svm = SVC(gamma='auto', kernel='rbf')  \n",
    "svm.fit(x_train, y_train) \n",
    "\n",
    "# Predict the test data \n",
    "predictions_SVM = svm.predict(x_test)\n",
    "\n",
    "# Accuracy of SVM\n",
    "#(1) JACCARD index\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print('jaccard_similarity_score of SVM = ', jaccard_similarity_score(y_test, predictions_SVM))\n",
    "\n",
    "# (2) F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1_score of SVM = ',f1_score(y_test, predictions_SVM, average='weighted'))\n",
    "\n",
    "\n",
    "# (3) LogLoss\n",
    "# a higher log-loss value means better predictions. \n",
    "# Log loss( Logarithmic loss) measures the performance of a \n",
    "#classifier where the predicted output is a probability value between 0 and 1.\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('log_loss of SVM = ',log_loss(y_test, predictions_SVM, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
    "\n",
    "###########################################################################################################################################################################\n",
    "\n",
    "#create model\n",
    "k = 9\n",
    "#Train Model \n",
    "neigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train) # training data of features and test data\n",
    "\n",
    "# Predict\n",
    "predictions_KNN = neigh.predict(x_test)\n",
    "\n",
    "# Accuracy of KNN\n",
    "#(1) JACCARD index\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "print('jaccard_similarity_score of KNN = ', jaccard_similarity_score(y_test, predictions_KNN))\n",
    "\n",
    "# (2) F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('f1_score of KNN = ',f1_score(y_test, predictions_KNN, average='weighted'))\n",
    "\n",
    "\n",
    "# (3) LogLoss\n",
    "# a higher log-loss value means better predictions. \n",
    "# Log loss( Logarithmic loss) measures the performance of a \n",
    "#classifier where the predicted output is a probability value between 0 and 1.\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print('log_loss of KNN = ',log_loss(y_test, predictions_KNN, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
