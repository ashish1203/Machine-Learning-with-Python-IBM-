#Load libraries
import pandas as pd # for df operation
import numpy as np  # for array

from sklearn import model_selection # train-test split
from sklearn.linear_model import LogisticRegression  
from sklearn.neighbors import KNeighborsClassifier   
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, export_graphviz

from sklearn.metrics import accuracy_score
#import matplotlib.pyplot as plt

#Load dataset from the Github
url = 'https://raw.githubusercontent.com/ashish1203/Machine-Learning-with-Python-IBM-/master/Loan-applicant-details_1.csv'
names = ['Loan_ID','Gender','Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History','Property_Area','Loan_Status']
df = pd.read_csv(url, names=names)

# visualize the data
#print(df.head(5)) # First 5 elements of the data
#print(df.describe()) # this will give the statistic data of data
#print(np.shape(df)) ## overall size data

# Check the type of data- Is any categorical data is there or not 
#print(df.dtypes) # this will give idea abt the data types involved in the dataset - row-wise


# Filter the data: changing the categorical data  to numerical value
from sklearn.preprocessing import LabelEncoder
var_mod = ['Loan_ID', 'Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']
le = LabelEncoder()
for i in var_mod:
    df[i] = le.fit_transform(df[i]) # encoding starts from 0
    
#again check the data
#print(df.dtypes)


#Split the dataset into 80% training and 20% test data
# for spliting the list - we first convert into array 
array = df.values
X = array[:,6:11] # all rows but 6:11 columns for training
X=X.astype('int')
Y = array[:,12] # 12 column select for - prediction

Y=Y.astype('int')

x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.2, random_state=4)

#print(x_train.shape) # 384 by 5
#print(x_test.shape) # 96 by 5
#print(y_train.shape) # 384
#print(y_test.shape) # 96

#####################################################################################################################################################################
# Training of the Model

# (1) Logistic Regression

# Model the logistic regression
# C:C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization.
# solver:numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.
# Regularization is a technique used to solve the overfitting problem in machine learning models.

model = LogisticRegression(C=0.01, solver='liblinear').fit(x_train,y_train)
model
#model = LogisticRegression()
#model.fit(x_train,y_train)
# Prediction of LR
predictions_LR = model.predict(x_test)

# Accuracy of LR
#(1) JACCARD index

from sklearn.metrics import jaccard_similarity_score
print('jaccard_similarity_score of LR = ', jaccard_similarity_score(y_test, predictions_LR))

# (2) F1 score
from sklearn.metrics import f1_score
print('f1_score of LR = ',f1_score(y_test, predictions_LR, average='weighted'))


# (3) LogLoss
# a higher log-loss value means better predictions. 
# Log loss( Logarithmic loss) measures the performance of a 
#classifier where the predicted output is a probability value between 0 and 1.

from sklearn.metrics import log_loss
print('log_loss of LR = ',log_loss(y_test, predictions_LR, eps=1e-15, normalize=True, sample_weight=None, labels=None))

##############################################################################################################################################################################
# (2) Decision Tree :We choose the Entropy as a model selection paramter.

model_DC = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
# fit the training data
model_DC.fit(x_train,y_train)

#Prediction of DC
predictions_DC = model_DC.predict(x_test)

# Accuracy of DC
#(1) JACCARD index

from sklearn.metrics import jaccard_similarity_score
print('jaccard_similarity_score of DC = ', jaccard_similarity_score(y_test, predictions_DC))

# (2) F1 score
from sklearn.metrics import f1_score
print('f1_score of DC = ',f1_score(y_test, predictions_DC, average='weighted'))


# (3) LogLoss
# a higher log-loss value means better predictions. 
# Log loss( Logarithmic loss) measures the performance of a 
#classifier where the predicted output is a probability value between 0 and 1.

from sklearn.metrics import log_loss
print('log_loss of DC = ',log_loss(y_test, predictions_DC, eps=1e-15, normalize=True, sample_weight=None, labels=None))

############################################################################################################################################################################

# (3) Support vector machine
# Model the SVM
svm = SVC(gamma='auto', kernel='rbf')  
svm.fit(x_train, y_train) 

# Predict the test data 
predictions_SVM = svm.predict(x_test)

# Accuracy of SVM
#(1) JACCARD index

from sklearn.metrics import jaccard_similarity_score
print('jaccard_similarity_score of SVM = ', jaccard_similarity_score(y_test, predictions_SVM))

# (2) F1 score
from sklearn.metrics import f1_score
print('f1_score of SVM = ',f1_score(y_test, predictions_SVM, average='weighted'))


# (3) LogLoss
# a higher log-loss value means better predictions. 
# Log loss( Logarithmic loss) measures the performance of a 
#classifier where the predicted output is a probability value between 0 and 1.

from sklearn.metrics import log_loss
print('log_loss of SVM = ',log_loss(y_test, predictions_SVM, eps=1e-15, normalize=True, sample_weight=None, labels=None))

###########################################################################################################################################################################

#create model
k = 9
#Train Model 
neigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train) # training data of features and test data

# Predict
predictions_KNN = neigh.predict(x_test)

# Accuracy of KNN
#(1) JACCARD index

from sklearn.metrics import jaccard_similarity_score
print('jaccard_similarity_score of KNN = ', jaccard_similarity_score(y_test, predictions_KNN))

# (2) F1 score
from sklearn.metrics import f1_score
print('f1_score of KNN = ',f1_score(y_test, predictions_KNN, average='weighted'))


# (3) LogLoss
# a higher log-loss value means better predictions. 
# Log loss( Logarithmic loss) measures the performance of a 
#classifier where the predicted output is a probability value between 0 and 1.

from sklearn.metrics import log_loss
print('log_loss of KNN = ',log_loss(y_test, predictions_KNN, eps=1e-15, normalize=True, sample_weight=None, labels=None))
