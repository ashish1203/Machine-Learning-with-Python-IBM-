{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (160, 7) (160,)\n",
      "Test set: (40, 7) (40,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference between Linear and Logistic Regression\n",
    "\n",
    "#Linear Regression is suited for estimating continuous values\n",
    "#For predicting the class of an observed data we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.\n",
    "\n",
    "#Logistic Regression :  It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
    "# Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate\n",
    "#into a probability with the following function, which is called sigmoid function ??:\n",
    "\n",
    "# Probability of class 1= P(Y=1|X) = sigma(theta(power(T)X) = e power(THETA(power(T)X1)/1+ e power(theta(power(T)x1)\n",
    "\n",
    "# sigma(theta(power(T)X)  ==> Sigmoid function ; theta(power(T)X) ==> Regression result\n",
    "\n",
    "# In brief: Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability\n",
    "# Main objective: is to find the best parameters ??:-n such a way that the model best predicts the class of each case.\n",
    "\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Load file\n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "#churn_df.head()\n",
    "#print(churn_df.dtypes)  # dtypes for -- data type\n",
    "\n",
    "# Feature selection\n",
    "Feature_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "Feature_df['churn'] = Feature_df['churn'].astype('int') # change the data type to int\n",
    "#Feature_df.head()\n",
    "#Feature_df.shape  # 200 by 10\n",
    "\n",
    "# Split the data\n",
    "# Input part\n",
    "X = np.asarray(Feature_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) # np.asarray :- convert the input into array\n",
    "#X[0:5]\n",
    "\n",
    "#target part\n",
    "y = np.asarray(Feature_df['churn'])\n",
    "#y[0:5]\n",
    "\n",
    "# Normalize the data\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X[0:5]\n",
    "\n",
    "#  Split the data into training and test part\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)\n",
    "\n",
    "# Model the logistic regression\n",
    "# C:C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization.\n",
    "# solver:numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.\n",
    "# Regularization is a technique used to solve the overfitting problem in machine learning models.\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "LR\n",
    "\n",
    "#Predict the data\n",
    "yhat = LR.predict(X_test)\n",
    "yhat\n",
    "\n",
    "#Predict probability\n",
    "yhat_prob = LR.predict_proba(X_test)  #predict_proba returns estimates for all classes, ordered by the label of classes\n",
    "#Assuming your target is (0,1), then the classifier would output a probability matrix of dimension (N,2). \n",
    "#The first index refers to the probability that the data belong to class 0, and the second refers to the \n",
    "#probability that the data belong to class 1\n",
    "yhat_prob\n",
    "\n",
    "#EVALUATION\n",
    "\n",
    "#(1) JACCARD index\n",
    "\n",
    "jaccard_similarity_score(y_test, yhat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
