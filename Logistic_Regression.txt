# Difference between Linear and Logistic Regression

#Linear Regression is suited for estimating continuous values
#For predicting the class of an observed data we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

#Logistic Regression :  It produces a formula that predicts the probability of the class label as a function of the independent variables.
# Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate
#into a probability with the following function, which is called sigmoid function ??:

# Probability of class 1= P(Y=1|X) = sigma(theta(power(T)X) = e power(THETA(power(T)X1)/1+ e power(theta(power(T)x1)

# sigma(theta(power(T)X)  ==> Sigmoid function ; theta(power(T)X) ==> Regression result

# In brief: Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability
# Main objective: is to find the best parameters ??:-n such a way that the model best predicts the class of each case.

import pandas as pd
import pylab as pl
import numpy as np
import scipy.optimize as opt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import jaccard_similarity_score
from sklearn.metrics import confusion_matrix
%matplotlib inline 
import matplotlib.pyplot as plt


#Load file
churn_df = pd.read_csv("ChurnData.csv")
#churn_df.head()
#print(churn_df.dtypes)  # dtypes for -- data type

# Feature selection
Feature_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]
Feature_df['churn'] = Feature_df['churn'].astype('int') # change the data type to int
#Feature_df.head()
#Feature_df.shape  # 200 by 10

# Split the data
# Input part
X = np.asarray(Feature_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']]) # np.asarray :- convert the input into array
#X[0:5]

#target part
y = np.asarray(Feature_df['churn'])
#y[0:5]

# Normalize the data
X = preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]

#  Split the data into training and test part
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

# Model the logistic regression
# C:C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization.
# solver:numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers.
# Regularization is a technique used to solve the overfitting problem in machine learning models.
LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
LR

#Predict the data
yhat = LR.predict(X_test)
yhat

#Predict probability
yhat_prob = LR.predict_proba(X_test)  #predict_proba returns estimates for all classes, ordered by the label of classes
#Assuming your target is (0,1), then the classifier would output a probability matrix of dimension (N,2). 
#The first index refers to the probability that the data belong to class 0, and the second refers to the 
#probability that the data belong to class 1
yhat_prob

#EVALUATION

#(1) JACCARD index

jaccard_similarity_score(y_test, yhat)

