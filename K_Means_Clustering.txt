#K-means clustering - with data 
#Customer Segmentation with K-Means- practice of partitioning a customer base into groups of individuals that have similar characteristics.

#Setting up K-Means: The KMeans class has many parameters that can be used, but we will be using these three:

#init: Initialization method of the centroids. #Value will be: "k-means++"==>#k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.
#n_clusters: The number of clusters to form as well as the number of centroids to generate.
#Value will be: 4 (since we have 4 centers)
#n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.


import pandas as pd
import numpy as np
# for normalization
from sklearn.preprocessing import StandardScaler   

from sklearn.cluster import KMeans #call k-means
import matplotlib.pyplot as plt 

# for 3D plot
from mpl_toolkits.mplot3d import Axes3D 
cust_data = pd.read_csv("Cust_Segmentation.csv")
#print(cust_data.head()) # first five features acroos rows n columns

# we always want to play with numbers (not with text data)
df = cust_data.drop('Address', axis=1) # drop column with axis label==> axis 1 indicates the columns, while axis 0
#df.head()


# data slicing in array =  [start : stop : steps] ==> forward [1:3:1] ==> backwards =[7:5:-1]==> array/List starts from 0 and last range element will not included in the list

#normalize the dataset==>interpret features with different magnitudes and distributions equally. 
new_data = df.values[:,1:] # rows, columns
a=new_data
#print (a)
area = np.pi * ( new_data[:, 1])**2  
plt.scatter(new_data[:, 0], new_data[:, 3], s=area,  alpha=0.5)
plt.xlabel('Age', fontsize=18)
plt.ylabel('Income', fontsize=16)

#plt.show()
# nan_to_num ==>  replace nan(Not A Number) with zero and inf with finite numbers in an array
new_data = np.nan_to_num(new_data) 
norm_data = StandardScaler().fit_transform(new_data)
norm_data

#Modeling the data set
##k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.
##n_clusters: The number of clusters to form as well as the number of centroids to generate.
##n_init: Number of time the k-means algorithm will be run with different centroid seeds.
clusterNum = 3
k_means = KMeans(init = "k-means++", n_clusters = clusterNum, n_init = 12)
k_means.fit(new_data)
labels = k_means.labels_
print(labels)
#print(np.shape(norm_data))
#print(np.shape(labels))

#Insights
df["Clus_km"] = labels
df.head(5)

# avergae value of each cluster
#print(df.groupby('Clus_km').mean())

#group by age 
#print(df.groupby(['Clus_km'])) 

# distribution of age vs income
#a1=new_data
#print(a1)

# 2-D plot
area = np.pi * ( new_data[:, 1])**2  
plt.scatter(new_data[:, 0],new_data[:, 1],  s=area, c=labels.astype(np.float), alpha=0.5)
plt.xlabel('Age', fontsize=18)
plt.ylabel('Income', fontsize=16)

plt.show()
#print(new_data)
#df.head()

#3-D plot :- we have to import axis3D
fig = plt.figure(1, figsize=(10, 6))  # length by width
plt.clf() #Clear the current figure.
ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=48, azim=134)  #(left, bottom, width, height) axes position.

plt.cla()

ax.set_xlabel('Education')
ax.set_ylabel('Age')
ax.set_zlabel('Income')

ax.scatter(new_data[:, 1], new_data[:, 0], new_data[:, 3], c= labels.astype(np.float))






